{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "\n",
    "import earthkit.data as ekd\n",
    "import earthkit.regrid as ekr\n",
    "\n",
    "from anemoi.inference.runners.simple import SimpleRunner\n",
    "from anemoi.inference.outputs.printer import print_state\n",
    "\n",
    "from ecmwf.opendata import Client as OpendataClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import initial conditions from ECMWF open data    \n",
    "**Parameters to retrieve from ECMWF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAM_SFC = [\"10u\", \"10v\", \"2d\", \"2t\", \"msl\", \"skt\", \"sp\", \"tcw\", \"lsm\", \"z\", \"slor\", \"sdor\"]\n",
    "# msl: Mean sea level pressure\n",
    "# skt: Skin temperature\n",
    "# sp: Surface pressure\n",
    "# tcw: Total column vertically-integrated water vapour\n",
    "# lsm: Land Sea Mask\n",
    "# z: Geopotential\n",
    "# slor: Slope of sub-gridscale orography (step 0)\n",
    "# sdor: Standard deviation of sub-gridscale orography (step 0)\n",
    "PARAM_SOIL =[\"vsw\",\"sot\"]\n",
    "# vsw: Volumetric soil water (layers 1-4)\n",
    "# sot: Soil temperature (layers 1-4)\n",
    "PARAM_PL = [\"gh\", \"t\", \"u\", \"v\", \"w\", \"q\"]\n",
    "# q: Specific humidity\n",
    "# w: vertical velocity\n",
    "LEVELS = [1000, 925, 850, 700, 600, 500, 400, 300, 250, 200, 150, 100, 50]\n",
    "SOIL_LEVELS = [1,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Date of initial conditions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial date is 2025-09-18 06:00:00\n"
     ]
    }
   ],
   "source": [
    "DATE = OpendataClient().latest()\n",
    "print(f\"Initial date is {DATE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fetch the data using ECMWF Open Data API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_open_data(param, levelist=[]):\n",
    "    fields = defaultdict(list)\n",
    "    # Get data at time t and t-1:\n",
    "    for date in [DATE - datetime.timedelta(hours=6), DATE]:\n",
    "        data = ekd.from_source(\"ecmwf-open-data\", date=date, param=param, levelist=levelist) # <class 'earthkit.data.readers.grib.file.GRIBReader'>\n",
    "        for f in data:  # <class 'earthkit.data.readers.grib.codes.GribField'>\n",
    "            assert f.to_numpy().shape == (721,1440)\n",
    "            values = np.roll(f.to_numpy(), -f.shape[1] // 2, axis=1)\n",
    "            # Interpolate the data to from 0.25°x0.25° (regular lat-lon grid, 2D) to N320 (reduced gaussian grid, 1D, see definition here: https://www.ecmwf.int/en/forecasts/documentation-and-support/gaussian_n320) \n",
    "            values = ekr.interpolate(values, {\"grid\": (0.25, 0.25)}, {\"grid\": \"N320\"})\n",
    "            # Add the values to the list\n",
    "            name = f\"{f.metadata('param')}_{f.metadata('levelist')}\" if levelist else f.metadata(\"param\")\n",
    "            fields[name].append(values)\n",
    "\n",
    "    # Create a single matrix for each parameter\n",
    "    for param, values in fields.items():\n",
    "        fields[param] = np.stack(values)\n",
    "\n",
    "    return fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Single-level field**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By downloading data from the ECMWF open data dataset, you agree to the terms: Attribution 4.0 International (CC BY 4.0). Please attribute ECMWF when downloading this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    }
   ],
   "source": [
    "fields.update(get_open_data(param=PARAM_SFC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    }
   ],
   "source": [
    "soil=get_open_data(param=PARAM_SOIL,levelist=SOIL_LEVELS)\n",
    "\n",
    "# soil parameters need to be renamed to be consistent with training\n",
    "mapping = {'sot_1': 'stl1', 'sot_2': 'stl2',\n",
    "           'vsw_1': 'swvl1','vsw_2': 'swvl2'}\n",
    "for k,v in soil.items():\n",
    "    fields[mapping[k]]=v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pressure level fields**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    }
   ],
   "source": [
    "fields.update(get_open_data(param=PARAM_PL, levelist=LEVELS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert geopotential height into geopotential (transform GH to Z)\n",
    "for level in LEVELS:\n",
    "    gh = fields.pop(f\"gh_{level}\")\n",
    "    fields[f\"z_{level}\"] = gh * 9.80665"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['10u', '10v', '2d', '2t', 'msl', 'skt', 'sp', 'tcw', 'lsm', 'z', 'slor', 'sdor', 'swvl1', 'swvl2', 'stl1', 'stl2', 't_1000', 't_925', 't_850', 't_700', 't_600', 't_500', 't_400', 't_300', 't_250', 't_200', 't_150', 't_100', 't_50', 'u_1000', 'u_925', 'u_850', 'u_700', 'u_600', 'u_500', 'u_400', 'u_300', 'u_250', 'u_200', 'u_150', 'u_100', 'u_50', 'v_1000', 'v_925', 'v_850', 'v_700', 'v_600', 'v_500', 'v_400', 'v_300', 'v_250', 'v_200', 'v_150', 'v_100', 'v_50', 'w_1000', 'w_925', 'w_850', 'w_700', 'w_600', 'w_500', 'w_400', 'w_300', 'w_250', 'w_200', 'w_150', 'w_100', 'w_50', 'q_1000', 'q_925', 'q_850', 'q_700', 'q_600', 'q_500', 'q_400', 'q_300', 'q_250', 'q_200', 'q_150', 'q_100', 'q_50', 'z_1000', 'z_925', 'z_850', 'z_700', 'z_600', 'z_500', 'z_400', 'z_300', 'z_250', 'z_200', 'z_150', 'z_100', 'z_50'])\n",
      "[[ 2.25539092  1.20666618  0.11760587 ... -6.39464971 -6.26019782\n",
      "  -5.23836346]\n",
      " [-1.10034716 -2.92889286 -4.08517912 ... -6.46423058 -6.38355944\n",
      "  -5.32138951]]\n"
     ]
    }
   ],
   "source": [
    "print(fields.keys())\n",
    "print(fields[\"10u\"])    # fields[\"field_name\"][0] array for t-1, fields[\"field_name\"][1] array for t, both in N320 format "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create initial state**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_state = dict(date=DATE, fields=fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load the model and run the forecast\n",
    "**Load model checkpoint form huggingface and create a runner**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {\"huggingface\":\"ecmwf/aifs-single-1.0\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = SimpleRunner(checkpoint, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the forecast**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Julie/Desktop/dev/regional_dd_WM/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 12 files: 100%|██████████| 12/12 [00:00<00:00, 152059.36it/s]\n",
      "/Users/Julie/Desktop/dev/regional_dd_WM/.venv/lib/python3.11/site-packages/anemoi/utils/config.py:209: UserWarning: Modifying an instance of DotDict(). This class is intended to be immutable.\n",
      "  warnings.warn(\"Modifying an instance of DotDict(). This class is intended to be immutable.\")\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'flash_attn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlead_time\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m12\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/dev/regional_dd_WM/.venv/lib/python3.11/site-packages/anemoi/inference/runner.py:131\u001b[39m, in \u001b[36mRunner.run\u001b[39m\u001b[34m(self, input_state, lead_time)\u001b[39m\n\u001b[32m    128\u001b[39m input_tensor = \u001b[38;5;28mself\u001b[39m.prepare_input_tensor(input_state)\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.postprocess(\u001b[38;5;28mself\u001b[39m.forecast(lead_time, input_tensor, input_state))\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m):\n\u001b[32m    133\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.report_error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/dev/regional_dd_WM/.venv/lib/python3.11/site-packages/anemoi/inference/postprocess.py:21\u001b[39m, in \u001b[36mNoop.__call__\u001b[39m\u001b[34m(self, source)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, source):\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m source\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/dev/regional_dd_WM/.venv/lib/python3.11/site-packages/anemoi/inference/runner.py:251\u001b[39m, in \u001b[36mRunner.forecast\u001b[39m\u001b[34m(self, lead_time, input_tensor_numpy, input_state)\u001b[39m\n\u001b[32m    250\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforecast\u001b[39m(\u001b[38;5;28mself\u001b[39m, lead_time, input_tensor_numpy, input_state):\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m.eval()\n\u001b[32m    253\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    255\u001b[39m     \u001b[38;5;66;03m# Create pytorch input tensor\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.13-macos-aarch64-none/lib/python3.11/functools.py:1001\u001b[39m, in \u001b[36mcached_property.__get__\u001b[39m\u001b[34m(self, instance, owner)\u001b[39m\n\u001b[32m    999\u001b[39m val = cache.get(\u001b[38;5;28mself\u001b[39m.attrname, _NOT_FOUND)\n\u001b[32m   1000\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[32m-> \u001b[39m\u001b[32m1001\u001b[39m     val = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1002\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1003\u001b[39m         cache[\u001b[38;5;28mself\u001b[39m.attrname] = val\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/dev/regional_dd_WM/.venv/lib/python3.11/site-packages/anemoi/inference/runner.py:241\u001b[39m, in \u001b[36mRunner.model\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;129m@cached_property\u001b[39m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmodel\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    240\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Timer(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.checkpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m         model = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m.to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m    242\u001b[39m         \u001b[38;5;66;03m# model.set_inference_options(**self.inference_options)\u001b[39;00m\n\u001b[32m    243\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/dev/regional_dd_WM/.venv/lib/python3.11/site-packages/torch/serialization.py:1530\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1528\u001b[39m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle.UnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1529\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle.UnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1530\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1531\u001b[39m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1532\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1533\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1534\u001b[39m \u001b[43m            \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1535\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1536\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[32m   1538\u001b[39m     f_name = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/dev/regional_dd_WM/.venv/lib/python3.11/site-packages/torch/serialization.py:2119\u001b[39m, in \u001b[36m_load\u001b[39m\u001b[34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[39m\n\u001b[32m   2117\u001b[39m \u001b[38;5;28;01mglobal\u001b[39;00m _serialization_tls\n\u001b[32m   2118\u001b[39m _serialization_tls.map_location = map_location\n\u001b[32m-> \u001b[39m\u001b[32m2119\u001b[39m result = \u001b[43munpickler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2120\u001b[39m _serialization_tls.map_location = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2122\u001b[39m torch._utils._validate_loaded_sparse_tensors()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/dev/regional_dd_WM/.venv/lib/python3.11/site-packages/torch/serialization.py:2108\u001b[39m, in \u001b[36m_load.<locals>.UnpicklerWrapper.find_class\u001b[39m\u001b[34m(self, mod_name, name)\u001b[39m\n\u001b[32m   2106\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m   2107\u001b[39m mod_name = load_module_mapping.get(mod_name, mod_name)\n\u001b[32m-> \u001b[39m\u001b[32m2108\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().find_class(mod_name, name)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'flash_attn'"
     ]
    }
   ],
   "source": [
    "for state in runner.run(input_state=input_state, lead_time=12):\n",
    "    print_state(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "regional-dd-wm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
